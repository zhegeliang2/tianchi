{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#分析top1中使用的特征，以及各个特征的相关性\n",
    "#coupon相关feature，这部分最直观\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "from datetime import date\n",
    "def get_rate(x):\n",
    "    #null代表没有折扣，即为1\n",
    "    if x == \"null\":\n",
    "        return 1\n",
    "    \n",
    "    #转换成折扣率\n",
    "    if \":\" in x:\n",
    "        return 1- float(x.split(\":\")[1])/float(x.split(\":\")[0])\n",
    "    else:\n",
    "        return float(x)\n",
    " \n",
    "def is_manjian(x):\n",
    "    x = x.split(\":\")\n",
    "    if len(x) == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "def get_man(x):\n",
    "    #没有满减的如何赋值？应该是比较大的值，因为满越大，大家点的概率越小，这个需要验证\n",
    "    #我认为这里赋值1为0不妥，因为这是成反比的，0会影响这个赋值\n",
    "    #赋值为null，便于树方法，线性模型如何处理的？,最后都有一个null替换成nan\n",
    "    if \":\" not in  x:\n",
    "        return \"null\"\n",
    "    \n",
    "    return int(x.split(\":\")[0])\n",
    "\n",
    "def get_jian(x):\n",
    "    #越小达标人家越不喜欢，赋值为0应该问题不大\n",
    "    if \":\" not in  x:\n",
    "        return \"null\"\n",
    "    \n",
    "    return int(x.split(\":\")[1])\n",
    "\n",
    "def get_dis(x):\n",
    "    #\n",
    "    if x == \"null\":\n",
    "        return \"null\"\n",
    "    \n",
    "    return x\n",
    "\n",
    "def get_weekday(x):\n",
    "    #如果是null,onehot时都设为0\n",
    "    if \"null\" == x:\n",
    "        return \"null\"\n",
    "    #20180401\n",
    "    return date(int(x[0:4]), int(x[4:6]), int(x[6:8])).weekday() + 1\n",
    "\n",
    "def get_weektype(x):\n",
    "    if x in [6,7]:\n",
    "        return 1\n",
    "    \n",
    "def get_monthday(x):\n",
    "    if \"null\" == x:\n",
    "        return \"null\"\n",
    "    \n",
    "    return int(x[6:8])\n",
    "    \n",
    "    return 0\n",
    "def coupon_feature(df):\n",
    "    #第一个就是折扣率，这个已经分析过，转换成折扣率，满额，减少额\n",
    "    df[\"c_discount_rate\"] = df.Discount_rate.apply(get_rate)\n",
    "    \n",
    "    #获取最大的满减，满值\n",
    "    #df.Discount_rate\n",
    "    #然后对于满减额，拆分成满和减，以及是否有满减\n",
    "    df[\"c_is_manjian\"] = df.Discount_rate.apply(is_manjian)\n",
    "    df[\"c_man\"] = df.Discount_rate.apply(get_man)\n",
    "    #max_man = df.c_man.max()\n",
    "    #df[df.c_man == 0][\"c_man\"] = max_man * 10 \n",
    "    df[\"c_jian\"] = df.Discount_rate.apply(get_jian)\n",
    "    \n",
    "    #然后就是距离，距离我认为由于有缺失值，onehot效果应该更好，待会可以验证\n",
    "    \n",
    "    #还有一个考虑到以后要用平均距离，最大距离这样的特征，貌似null替换成nan，然后mean操作可以忽略掉\n",
    "    df[\"distance\"] = df.Distance.apply(get_dis)\n",
    "    df[\"c_distance\"] = df.Distance.apply(get_dis)\n",
    "    #不保留null，对应lr模型有用\n",
    "    discols = [\"c_dis_%s\" %(i) for i in range(0,11)]\n",
    "    tmpdf = pd.get_dummies(df[\"distance\"].replace(\"null\", np.nan))\n",
    "    tmpdf.columns = discols\n",
    "    df[discols] = tmpdf   \n",
    "    \n",
    "    #时间特征，主要就是接收时间，是一周的第几天，一个月的第几天，是否是周末\n",
    "    df[\"weekday\"] = df.Date_received.astype('str').apply(get_weekday)\n",
    "    df[\"c_weektype\"] = df.weekday.apply(get_weektype)\n",
    "    #onehot编码，对于lr模型比较有用\n",
    "    weekdaycols = [\"c_weekday_%s\" %(i) for i in range(1,8)]\n",
    "    tmpdf = pd.get_dummies(df[\"weekday\"].replace(\"null\", np.nan))\n",
    "    tmpdf.columns = weekdaycols\n",
    "    df[weekdaycols] = tmpdf\n",
    "    #\n",
    "    d = df[['Coupon_id']]\n",
    "    d['c_coupon_count'] = 1\n",
    "    d = d.groupby('Coupon_id').agg('sum').reset_index()\n",
    "    #coupon_count，每个coupon出现的数目\n",
    "    #df = pd.merge(df,d,on='Coupon_id',how='left')    \n",
    "    #df[\"c_monthday\"] = df.Date_received.apply(get_monthday)\n",
    "    #加上onehot\n",
    "    #monthdaycols = [\"c_monthday_%s\" %(i) for i in range(1,32)]\n",
    "    #tmpdf = pd.get_dummies(df[\"c_monthday\"].replace(\"null\",np.nan), prefix=\"c_monthday_\")\n",
    "    #df[tmpdf.columns] = tmpdf\n",
    "    #replace null with nan\n",
    "    #df.to_csv(\"./data.csv\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#开始预测，生辰标签\n",
    "def get_label(x):\n",
    "    #没有券的是-1\n",
    "    if (x.Date_received == \"null\"):\n",
    "        return -1\n",
    "    #如果购买了\n",
    "    if x.Date != 'null':\n",
    "        #而且购买日期和领券日期相差不超过15天，值为1\n",
    "        tmp = pd.to_datetime(x.Date, format=\"%Y%m%d\") - pd.to_datetime(x.Date_received, format=\"%Y%m%d\")\n",
    "        if tmp < pd.Timedelta(15, 'D'):\n",
    "            return 1    \n",
    "    return 0\n",
    "    #如果有券购买的算是1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#user realted\n",
    "def user_feature(df):\n",
    "    #用户相关的特征，考虑这么几个特征\n",
    "    u = df[[\"User_id\"]]\n",
    "    u.drop_duplicates(inplace=True)\n",
    "    \n",
    "    #用户总的购买次数\n",
    "    u1 = df[df.Date != 'null'][['User_id']]\n",
    "    u1[\"u_buy_count\"] = 1\n",
    "    u1 = u1.groupby(\"User_id\", as_index=False).count()\n",
    "    \n",
    "    #用户总的优惠券购买次数\n",
    "    u2 = df[(df.Date != 'null') & (df.Date_received != 'null')][['User_id']]\n",
    "    u2[\"u_coupon_buy_count\"] = 1\n",
    "    u2 = u2.groupby(\"User_id\", as_index=False).agg('sum'\n",
    "                                                  )\n",
    "    #用户总的获取优惠券的次数\n",
    "    u3 = df[(df.Date_received != 'null')][['User_id']]\n",
    "    u3[\"u_coupon_count\"] = 1\n",
    "    u3 = u3.groupby(\"User_id\", as_index=False).count()       \n",
    "    #用户优惠券使用占比\n",
    "    #用户优惠券购买占比\n",
    "    \n",
    "    #用户相关的折扣率，平均折扣率，这个总有值\n",
    "    #这个作为特征反而降低了预测准确率，应该和改值的准确度有一定关系\n",
    "    u4 = df[(df.Date != 'null') & (df.Date_received != 'null')][['User_id', \"c_discount_rate\"]]\n",
    "    u4 = u4.groupby(\"User_id\", as_index=False).agg('mean')\n",
    "    u4.rename(columns={\"c_discount_rate\":\"u_mean_discount\"}, inplace=True)\n",
    "    \n",
    "    \n",
    "    #用户相关的距离,平均距离，最小，最大，平均，中位数\n",
    "    tmpdf = df[(df.Date != 'null') & (df.Date_received != 'null')][['User_id', \"Distance\"]].copy()\n",
    "    tmpdf.Distance.replace(\"null\", -1, inplace=True)\n",
    "    tmpdf.Distance = tmpdf.Distance.astype(int)\n",
    "    tmpdf.replace(-1, np.nan, inplace=True)\n",
    "    #\n",
    "    u5 = tmpdf.groupby(\"User_id\", as_index=False).agg('mean')\n",
    "    u5.rename(columns = {'Distance':\"u_mean_distance\"}, inplace=True)\n",
    "    #\n",
    "    u6 = tmpdf.groupby(\"User_id\", as_index=False).agg('max')\n",
    "    u6.rename(columns = {'Distance':\"u_max_distance\"}, inplace=True)    \n",
    "    u7= tmpdf.groupby(\"User_id\", as_index=False).agg('min')\n",
    "    u7.rename(columns = {'Distance':\"u_min_distance\"}, inplace=True)   \n",
    "    u8= tmpdf.groupby(\"User_id\", as_index=False).agg('median')\n",
    "    u8.rename(columns = {'Distance':\"u_median_distance\"}, inplace=True)  \n",
    "    \n",
    "    #用户总的购买商户数，代表购买的不同商户个数\n",
    "    u9 = df[df.Date != 'null'][['User_id', 'Merchant_id']]\n",
    "    u9.drop_duplicates(inplace=True);\n",
    "    u9[\"u_merchant_count\"] = 1\n",
    "    u9 = u9.groupby('User_id', as_index=False).agg(sum)\n",
    "    u9 = u9[['User_id', 'u_merchant_count']]\n",
    "    #用户从收券到消费的时间段\n",
    "    u10 = df[(df.Date != 'null') & (df.Date_received !='null')][['User_id', 'Date_received', 'Date']]\n",
    "    u10[\"date_datereceived\"] = u10.Date_received.astype('str') + \":\" + u10.Date.astype('str') \n",
    "    u10[\"date_gap\"] = u10.date_datereceived.apply(get_gap)\n",
    "    u10 = u10[[\"User_id\", \"date_gap\"]]\n",
    "    \n",
    "    u11 = u10.groupby(\"User_id\", as_index=False).mean()\n",
    "    u11.rename(columns={\"date_gap\":\"u_date_gap_mean\"}, inplace=True)\n",
    "    u12 = u10.groupby(\"User_id\", as_index=False).min()\n",
    "    u12.rename(columns={\"date_gap\":\"u_date_gap_min\"}, inplace=True)   \n",
    "    u13 = u10.groupby(\"User_id\", as_index=False).max()\n",
    "    u13.rename(columns={\"date_gap\":\"u_date_gap_max\"}, inplace=True)     \n",
    "    #用户相关的日期统计，是否是喜欢周末消费\n",
    "    \n",
    "    user_feature = pd.merge(u, u1, on=[\"User_id\"], how='left')\n",
    "    user_feature = pd.merge(user_feature, u2, on=[\"User_id\"],how='left')\n",
    "    user_feature = pd.merge(user_feature, u3, on=[\"User_id\"], how='left')\n",
    "    user_feature[\"u_coupon_buy_rate\"] = user_feature.u_coupon_buy_count.astype(float)/user_feature.u_buy_count.astype(float)\n",
    "    user_feature[\"u_coupon_use_rate\"] = user_feature.u_coupon_buy_count.astype(float)/user_feature.u_coupon_count.astype(float)\n",
    "    #user_feature = pd.merge(user_feature, u4, on=[\"User_id\"], how='left')\n",
    "    user_feature = pd.merge(user_feature, u5, on=[\"User_id\"], how='left')\n",
    "    user_feature = pd.merge(user_feature, u6, on=[\"User_id\"], how='left')\n",
    "    user_feature = pd.merge(user_feature, u7, on=[\"User_id\"], how='left')\n",
    "    user_feature = pd.merge(user_feature, u8, on=[\"User_id\"], how='left')\n",
    "    user_feature = pd.merge(user_feature, u9, on=[\"User_id\"], how='left')\n",
    "    user_feature = pd.merge(user_feature, u11, on=[\"User_id\"], how='left')\n",
    "    user_feature = pd.merge(user_feature, u12, on=[\"User_id\"], how='left')\n",
    "    user_feature = pd.merge(user_feature, u13, on=[\"User_id\"], how='left')    \n",
    "    #填充空值\n",
    "    user_feature = user_feature.fillna(0)\n",
    "    return user_feature\n",
    "def get_gap(x):\n",
    "    d1,d2 = x.split(\":\")\n",
    "    return (date(int(d2[0:4]), int(d2[4:6]), int(d2[6:8])) - date(int(d1[0:4]), int(d1[4:6]), int(d1[6:8]))).days\n",
    "\n",
    "def merchant_feature(df):\n",
    "    #用户相关的特征，考虑这么几个特征\n",
    "    m = df[[\"Merchant_id\"]]\n",
    "    m.drop_duplicates(inplace=True)\n",
    "    \n",
    "    #merchant总的被购买次数\n",
    "    m1 = df[df.Date != 'null'][['Merchant_id']]\n",
    "    m1[\"m_buy_count\"] = 1\n",
    "    m1 = m1.groupby(\"Merchant_id\", as_index=False).count()\n",
    "    \n",
    "    #merchant总的优惠券购买次数\n",
    "    m2 = df[(df.Date != 'null') & (df.Date_received != 'null')][['Merchant_id']]\n",
    "    m2[\"m_coupon_buy_count\"] = 1\n",
    "    m2 = m2.groupby(\"Merchant_id\", as_index=False).count()    \n",
    "    #用户总的获取优惠券的次数\n",
    "    m3 = df[(df.Date_received != 'null')][['Merchant_id']]\n",
    "    m3[\"m_coupon_count\"] = 1\n",
    "    m3 = m3.groupby(\"Merchant_id\", as_index=False).count()       \n",
    "    #用户优惠券使用占比\n",
    "    #用户优惠券购买占比\n",
    "    \n",
    "    #用户相关的折扣率，平均折扣率，这个总有值\n",
    "    #m4 = df[(df.Date != 'null') & (df.Date_received != 'null')][['Merchant_id', \"c_discount_rate\"]]\n",
    "    #m4 = m4.groupby(\"Merchant_id\", as_index=False).agg('mean')\n",
    "    #m4.rename(columns={\"c_discount_rate\":\"m_mean_discount\"}, inplace=True)\n",
    "    \n",
    "    \n",
    "    #用户相关的距离,平均距离，最小，最大，平均，中位数\n",
    "    tmpdf = df[(df.Date != 'null') & (df.Date_received != 'null')][['Merchant_id', \"Distance\"]].copy()\n",
    "    tmpdf.Distance.replace(\"null\", -1, inplace=True)\n",
    "    tmpdf.Distance = tmpdf.Distance.astype(int)\n",
    "    tmpdf.replace(-1, np.nan, inplace=True)\n",
    "    #\n",
    "    m5 = tmpdf.groupby(\"Merchant_id\", as_index=False).agg('mean')\n",
    "    m5.rename(columns = {'Distance':\"m_mean_distance\"}, inplace=True)\n",
    "    #\n",
    "    m6 = tmpdf.groupby(\"Merchant_id\", as_index=False).agg('max')\n",
    "    m6.rename(columns = {'Distance':\"m_max_distance\"}, inplace=True)    \n",
    "    m7= tmpdf.groupby(\"Merchant_id\", as_index=False).agg('min')\n",
    "    m7.rename(columns = {'Distance':\"m_min_distance\"}, inplace=True)   \n",
    "    m8= tmpdf.groupby(\"Merchant_id\", as_index=False).agg('median')\n",
    "    m8.rename(columns = {'Distance':\"m_median_distance\"}, inplace=True)  \n",
    "    \n",
    "    #merchant总的coupon数\n",
    "    #m9 = df[df.Coupon_id != 'null'][['Coupon_id', 'Merchant_id']]\n",
    "    #m9.drop_duplicates(inplace=True);\n",
    "    #m9[\"m_merchant_coupon_count\"] = 1\n",
    "    #m9 = m9.groupby('Merchant_id', as_index=False).agg(sum)\n",
    "    #m9 = m9[['Merchant_id', 'm_merchant_coupon_count']]\n",
    "    #用户从收券到消费的时间段\n",
    "    m10 = df[(df.Date != 'null') & (df.Date_received !='null')][['Merchant_id', 'Date_received', 'Date']]\n",
    "    m10[\"date_datereceived\"] =  m10.Date_received + \":\" +  m10.Date\n",
    "    m10[\"date_gap\"] = m10.date_datereceived.apply(get_gap)\n",
    "    m10 = m10[[\"Merchant_id\", \"date_gap\"]]\n",
    "    \n",
    "    m11 = m10.groupby(\"Merchant_id\", as_index=False).mean()\n",
    "    m11.rename(columns={\"date_gap\":\"m_date_gap_mean\"}, inplace=True)\n",
    "    m12 = m10.groupby(\"Merchant_id\", as_index=False).min()\n",
    "    m12.rename(columns={\"date_gap\":\"m_date_gap_min\"}, inplace=True)   \n",
    "    m13 = m10.groupby(\"Merchant_id\", as_index=False).max()\n",
    "    m13.rename(columns={\"date_gap\":\"m_date_gap_max\"}, inplace=True)     \n",
    "    #用户相关的日期统计\n",
    "    m_feature = pd.merge(m, m1, on=[\"Merchant_id\"], how='left')\n",
    "    m_feature = pd.merge(m_feature, m2, on=[\"Merchant_id\"],how='left')\n",
    "    m_feature = pd.merge(m_feature, m3, on=[\"Merchant_id\"], how='left')\n",
    "    m_feature[\"m_coupon_buy_rate\"] = m_feature.m_coupon_buy_count.astype(float)/m_feature.m_buy_count.astype(float)\n",
    "    m_feature[\"m_coupon_use_rate\"] = m_feature.m_coupon_buy_count.astype(float)/m_feature.m_coupon_count.astype(float)\n",
    "    #先不考虑平均折扣率\n",
    "    #m_feature = pd.merge(m_feature, m4, on=[\"Merchant_id\"], how='left')\n",
    "    m_feature = pd.merge(m_feature, m5, on=[\"Merchant_id\"], how='left')\n",
    "    m_feature = pd.merge(m_feature, m6, on=[\"Merchant_id\"], how='left')\n",
    "    m_feature = pd.merge(m_feature, m7, on=[\"Merchant_id\"], how='left')\n",
    "    m_feature = pd.merge(m_feature, m8, on=[\"Merchant_id\"], how='left')\n",
    "    #m9和m3是重复的\n",
    "    #m_feature = pd.merge(m_feature, m9, on=[\"Merchant_id\"], how='left')\n",
    "    #先参考top1\n",
    "    m_feature = pd.merge(m_feature, m11, on=[\"Merchant_id\"], how='left')\n",
    "    m_feature = pd.merge(m_feature, m12, on=[\"Merchant_id\"], how='left')\n",
    "    m_feature = pd.merge(m_feature, m13, on=[\"Merchant_id\"], how='left')    \n",
    "    #填充空值\n",
    "    m_feature = m_feature.fillna(0)\n",
    "    return m_feature    \n",
    "    #merchant的特征\n",
    "    \n",
    "    #merchant被购买的次数\n",
    "    #merchant被领取券的次数\n",
    "    #merchant被优惠券购买的次数\n",
    "    #两个概率\n",
    "    \n",
    "    #merchant的折扣率\n",
    "    \n",
    "    #merchant的距离\n",
    "    \n",
    "    #merchant的被消费时间\n",
    "    \n",
    "def user_merchant(df):\n",
    "    #用户和商品的交叉特征\n",
    "    \n",
    "    um = df[[\"User_id\", \"Merchant_id\"]]\n",
    "    um.drop_duplicates(inplace=True)\n",
    "    \n",
    "    #用户对某商品的购买次数\n",
    "    um1 = df[df.Date !='null'][['User_id', 'Merchant_id']]\n",
    "    um1[\"um_buy_count\"] = 1\n",
    "    um1 = um1.groupby([\"User_id\", \"Merchant_id\"], as_index=False).agg('sum')\n",
    "    \n",
    "    #用户使用优惠券对某商品的购买次数\n",
    "    um2 = df[(df.Date !='null') & (df.Date_received != 'null')][['User_id', 'Merchant_id']]\n",
    "    um2[\"um_buy_with_coupon_count\"] = 1\n",
    "    um2 = um2.groupby([\"User_id\", \"Merchant_id\"], as_index=False).agg('sum')\n",
    "    \n",
    "    #用户总共收到的优惠券的数目\n",
    "    um3 = df[df.Date_received !='null'][['User_id', 'Merchant_id']]\n",
    "    um3[\"um_coupon_count\"] = 1\n",
    "    um3 = um3.groupby([\"User_id\", \"Merchant_id\"], as_index=False).agg('sum')\n",
    " \n",
    "    #交互次数\n",
    "    um4 = df[['User_id', 'Merchant_id']]\n",
    "    um4[\"um_count\"] = 1\n",
    "    um4 = um4.groupby([\"User_id\", \"Merchant_id\"], as_index=False).agg('sum')\n",
    "    \n",
    "    #common rate，消费了但没有使用优惠券\n",
    "    um5 = df[(df.Date !='null') &(df.Coupon_id == 'null')][['User_id', 'Merchant_id']]\n",
    "    um5[\"um_common_buy_count\"] = 1\n",
    "    um5 = um5.groupby([\"User_id\", \"Merchant_id\"], as_index=False).agg('sum')\n",
    "    \n",
    "    #用户总共收到\n",
    "    um_feature = pd.merge(um, um1, on=[\"User_id\", 'Merchant_id'], how='left')\n",
    "    um_feature = pd.merge(um_feature, um2, on=[\"User_id\", 'Merchant_id'], how='left')\n",
    "    um_feature = pd.merge(um_feature, um3, on=[\"User_id\", 'Merchant_id'], how='left')\n",
    "    um_feature = pd.merge(um_feature, um4, on=[\"User_id\", 'Merchant_id'], how='left')\n",
    "    um_feature = pd.merge(um_feature, um5, on=[\"User_id\", 'Merchant_id'], how='left')\n",
    "    um_feature[\"um_buy_rate\"] = um_feature.um_buy_count.astype('float')/um_feature.um_count.astype('float')\n",
    "    um_feature[\"um_buy_with_coupon_rate\"] = um_feature.um_buy_with_coupon_count.astype('float')/um_feature.um_buy_count.astype('float')\n",
    "    um_feature[\"um_coupon_transfer_rate\"] = um_feature.um_buy_with_coupon_count.astype('float')/um_feature.um_coupon_count.astype('float')\n",
    "    um_feature[\"um_common_buy_rate\"] = um_feature.um_common_buy_count.astype('float')/um_feature.um_buy_count.astype('float')\n",
    "    um_feature = um_feature.fillna(0)\n",
    "    \n",
    "    return um_feature\n",
    "\n",
    "#泄漏特征\n",
    "def other_feature(test):\n",
    "    #主要出发点就是test中如果同一用户收到同一优惠券次数越多，越表示这个券被消费了\n",
    "    t = test[[\"User_id\", \"Coupon_id\"]]\n",
    "    t.drop_duplicates(inplace=True)\n",
    "    \n",
    "    #用户接受到总的券的数目\n",
    "    t1 = test[[\"User_id\"]]\n",
    "    t1[\"t_total_coupon_count\"] = 1\n",
    "    t1 = t1.groupby(\"User_id\", as_index=False).agg('sum')\n",
    "    \n",
    "    #用户同一个券的接收次数\n",
    "    t2 = test[[\"User_id\", 'Coupon_id']]\n",
    "    t2[\"t_same_coupon_count\"] = 1\n",
    "    t2 = t2.groupby([\"User_id\", \"Coupon_id\"], as_index=False).agg('sum')\n",
    "    t2[\"t_same_coupon_morethan1\"]  = t2.t_same_coupon_count.apply(lambda x: 1 if x>1 else 0)\n",
    "    \n",
    "    #同一优惠券的最近接收日期和最小接收日期\n",
    "    t3 = test[[\"User_id\", \"Coupon_id\", \"Date_received\"]]\n",
    "    t3.Date_received= t3.Date_received.astype('str')\n",
    "    t3 = t3.groupby(['User_id','Coupon_id'])['Date_received'].agg(lambda x:':'.join(x)).reset_index()\n",
    "    t3['receive_number'] = t3.Date_received.apply(lambda s:len(s.split(':')))\n",
    "    t3 = t3[t3.receive_number>1]\n",
    "    t3[\"t_max_coupon_time\"]= t3.Date_received.apply(lambda x:max([int(i) for i in x.split(\":\")]))\n",
    "    t3[\"t_min_coupon_time\"]= t3.Date_received.apply(lambda x:min([int(i) for i in x.split(\":\")]))\n",
    "    t3 = t3[[\"User_id\", \"Coupon_id\", \"t_max_coupon_time\", \"t_min_coupon_time\"]]\n",
    "    #获取券和最小和最大接收时间的差值\n",
    "    t4 = test[[\"User_id\", \"Coupon_id\", \"Date_received\"]]\n",
    "    t4 = pd.merge(t4, t3, on=[\"User_id\", \"Coupon_id\"], how='left')\n",
    "    t4[\"t_user_receive_same_coupon_lastone\"] = t4.t_max_coupon_time-t4.Date_received.astype('int')\n",
    "    t4[\"t_user_receive_same_coupon_firstone\"] = t4.Date_received.astype('int') - t4.t_min_coupon_time\n",
    "    #\n",
    "    def is_firstlastone(x):\n",
    "        if x==0:\n",
    "            return 1\n",
    "        elif x>0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1 #those only receive once\n",
    "    \n",
    "    t4[\"t_user_receive_same_coupon_lastone\"] = t4[\"t_user_receive_same_coupon_lastone\"].apply(is_firstlastone)\n",
    "    t4[\"t_user_receive_same_coupon_firstone\"] = t4[\"t_user_receive_same_coupon_firstone\"].apply(is_firstlastone)\n",
    "    t4 = t4[[\"User_id\", \"Coupon_id\",  \"Date_received\", \"t_user_receive_same_coupon_lastone\", \"t_user_receive_same_coupon_firstone\"]]\n",
    "    #用户同一天收到的优惠券数目\n",
    "    t5 = test[[\"User_id\", \"Date_received\"]]\n",
    "    t5[\"t_day_received_all_coupon_num\"] = 1\n",
    "    t5 = t5.groupby(['User_id', 'Date_received'], as_index=False).agg('sum')\n",
    "    \n",
    "    #同一天同一优惠券的数目\n",
    "    t6 = test[[\"User_id\", \"Coupon_id\", \"Date_received\"]]\n",
    "    t6[\"t_day_received_same_coupon_num\"] = 1\n",
    "    t6 = t6.groupby([\"User_id\", \"Coupon_id\", \"Date_received\"], as_index=False).agg('sum')\n",
    "    \n",
    "    #\n",
    "    t7 = test[['User_id', 'Coupon_id', 'Date_received']]\n",
    "    t7.Date_received = t7.Date_received.astype('str')\n",
    "    t7 = t7.groupby(['User_id','Coupon_id'])['Date_received'].agg(lambda x:':'.join(x)).reset_index()\n",
    "    t7.rename(columns={'Date_received':'dates'},inplace=True)\n",
    "    t8 = test[['User_id','Coupon_id','Date_received']]\n",
    "    t8 = pd.merge(t8,t7,on=['User_id','Coupon_id'],how='left')\n",
    "    t8['date_received_date'] = t8.Date_received.astype('str') + '-' + t8.dates\n",
    "    t8['t_day_gap_before'] = t8.date_received_date.apply(get_day_gap_before)\n",
    "    t8['t_day_gap_after'] = t8.date_received_date.apply(get_day_gap_after)\n",
    "    t8 = t8[['User_id','Coupon_id','Date_received','t_day_gap_before','t_day_gap_after']]\n",
    "    \n",
    "    #\n",
    "    other = pd.merge(t1, t2, on=[\"User_id\"])\n",
    "    other = pd.merge(other,t4, on=[\"User_id\", \"Coupon_id\"])\n",
    "    other = pd.merge(other,t5, on=[\"User_id\", \"Date_received\"])\n",
    "    other = pd.merge(other,t6, on=[\"User_id\", \"Coupon_id\", \"Date_received\"])\n",
    "    other = pd.merge(other,t8, on=[\"User_id\", \"Coupon_id\", \"Date_received\"])\n",
    "    #\n",
    "    \n",
    "    return other\n",
    "    \n",
    "def is_firstlastone(x):\n",
    "    if x==0:\n",
    "        return 1\n",
    "    elif x>0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1 #those only receive once\n",
    "from datetime import date\n",
    "def get_day_gap_before(s):\n",
    "    date_received,dates = s.split('-')\n",
    "    dates = dates.split(':')\n",
    "    gaps = []\n",
    "    #对于每个消费日期，计算收到券的日期和消费时间的差值，如果消费后收券，这个间隔记做gap\n",
    "    for d in dates:\n",
    "        this_gap = (date(int(date_received[0:4]),int(date_received[4:6]),int(date_received[6:8]))-date(int(d[0:4]),int(d[4:6]),int(d[6:8]))).days\n",
    "        if this_gap>0:\n",
    "            gaps.append(this_gap)\n",
    "    if len(gaps)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(gaps)\n",
    "def get_day_gap_after(s):\n",
    "    date_received,dates = s.split('-')\n",
    "    dates = dates.split(':')\n",
    "    gaps = []\n",
    "    #收券后消费的间隔的最小值\n",
    "    for d in dates:\n",
    "        this_gap = (date(int(d[0:4]),int(d[4:6]),int(d[6:8]))-date(int(date_received[0:4]),int(date_received[4:6]),int(date_received[6:8]))).days\n",
    "        if this_gap>0:\n",
    "            gaps.append(this_gap)\n",
    "    if len(gaps)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(gaps)   \n",
    "def usercouponFeature(df):\n",
    "\n",
    "    dftmp = df[df.Coupon_id != 'null'].copy()\n",
    "    \n",
    "    \n",
    "    uc = dftmp[['User_id', 'Coupon_id']].copy().drop_duplicates()\n",
    "\n",
    "    #这是用户领取券的数目（包括消费的和未消费的）\n",
    "    uc1 = dftmp[['User_id', 'Coupon_id']].copy()\n",
    "    uc1['uc_count'] = 1\n",
    "    uc1 = uc1.groupby(['User_id', 'Coupon_id'], as_index = False).count()\n",
    "\n",
    "    #用户消费的券的数目\n",
    "    uc2 = dftmp[dftmp['Date'] != 'null'][['User_id', 'Coupon_id']].copy()\n",
    "    uc2['uc_coupon_buy_count'] = 1\n",
    "    uc2 = uc2.groupby(['User_id', 'Coupon_id'], as_index = False).count()\n",
    "\n",
    "    #接受了但是没消费的\n",
    "    uc3 = dftmp[(dftmp['Date_received'] != 'null') & (dftmp[\"Date\"] == \"null\")][['User_id', 'Coupon_id']].copy()\n",
    "    uc3['uc_coupon_nosale_count'] = 1\n",
    "    uc3 = uc3.groupby(['User_id', 'Coupon_id'], as_index = False).count()\n",
    "\n",
    "\n",
    "    user_coupon_feature = pd.merge(uc, uc1, on = ['User_id','Coupon_id'], how = 'left')\n",
    "    user_coupon_feature = pd.merge(user_coupon_feature, uc2, on = ['User_id','Coupon_id'], how = 'left')\n",
    "    user_coupon_feature = pd.merge(user_coupon_feature, uc3, on = ['User_id','Coupon_id'], how = 'left')\n",
    "    user_coupon_feature = user_coupon_feature.fillna(0)\n",
    "\n",
    "    user_coupon_feature['uc_buy_rate'] = user_coupon_feature['uc_coupon_buy_count'].astype('float')/user_coupon_feature['uc_count'].astype('float')\n",
    "    user_coupon_feature['uc_no_buy_rate'] = user_coupon_feature['uc_coupon_buy_count'].astype('float')/user_coupon_feature['uc_count'].astype('float')\n",
    "    user_coupon_feature = user_coupon_feature.fillna(0)\n",
    "\n",
    "    print(user_coupon_feature.columns.tolist())\n",
    "    return user_coupon_feature    \n",
    "def featureProcess(feature, train, test):\n",
    "    \"\"\"\n",
    "    feature engineering from feature data\n",
    "    then assign user, merchant, and user_merchant feature for train and test \n",
    "    \"\"\"\n",
    "    \n",
    "    uf = user_feature(feature)\n",
    "    mf = merchant_feature(feature)\n",
    "    um = user_merchant(feature)\n",
    "    #训练集合coupon和测试集合coupon没有交集，所以不适用user和coupon的交集了\n",
    "    other = other_feature(test)\n",
    "    #加上user copon\n",
    "    #uc = usercouponFeature(feature)\n",
    "    \n",
    "\n",
    "    train = pd.merge(train, uf, on=[\"User_id\"], how='left')\n",
    "    train = pd.merge(train, mf, on=[\"Merchant_id\"], how='left')\n",
    "    train = pd.merge(train, um, on=[ \"User_id\", \"Merchant_id\"], how='left')\n",
    "    train = pd.merge(train, other, on=[\"User_id\", 'Coupon_id', \"Date_received\"], how='left')\n",
    "    #train = pd.merge(train, uc, on=[ \"User_id\", \"Coupon_id\"], how='left')\n",
    "    train = train.replace(\"null\", np.nan)\n",
    "    #\n",
    "    train.drop_duplicates(inplace=True)\n",
    "    train = train.fillna(0)\n",
    "    #\n",
    "    \n",
    "    #test = pd.merge(test, uf, on = 'User_id', how = 'left')   \n",
    "    #test = pd.merge(test, mf, on = 'Merchant_id', how = 'left')   \n",
    "    #test = pd.merge(test, um, on=[\"User_id\", \"Merchant_id\"], how='left')\n",
    "    #test = pd.merge(test, uc, on=[ \"User_id\", \"Coupon_id\"], how='left')\n",
    "    #test = pd.merge(test, other, on=[\"User_id\", 'Coupon_id', \"Date_received\"], how='left')\n",
    "    #test = test.replace(\"null\", np.nan)\n",
    "    #\n",
    "    #test.drop_duplicates(inplace=True)\n",
    "    #test = test.fillna(0)\n",
    "    \n",
    "    return train\n",
    "#生成泄漏信息\n",
    "def otherFeature(train, test):\n",
    "    #\n",
    "    other = other_feature(test)\n",
    "    train = pd.merge(train, other, on=[\"User_id\", 'Coupon_id', \"Date_received\"], how='left')\n",
    "    train = train.fillna(0)\n",
    "    test = pd.merge(test, other, on=[\"User_id\", 'Coupon_id', \"Date_received\"], how='left')\n",
    "    test = test.fillna(0)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dfoff = pd.read_csv(\"ccf_offline_stage1_train.csv\")\n",
    "dftest = pd.read_csv('ccf_offline_stage1_test_revised.csv')\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = dfoff[[\"User_id\", \"Merchant_id\", \"Date\", \"Date_received\"]]\n",
    "tmp = tmp[(tmp.Date != 'null') & (tmp.Date_received != 'null')]\n",
    "tmp[\"um_count\"]=1\n",
    "tmp = tmp.groupby([\"User_id\", \"Merchant_id\"], as_index=False).agg('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuyanlei/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "tmp1 = dfoff[[\"User_id\", \"Merchant_id\", \"Date\", \"Date_received\"]]\n",
    "#tmp = tmp[(tmp.Date != 'null') & (tmp.Date_received != 'null')]\n",
    "tmp1[\"um_count\"]=1\n",
    "tmp1 = tmp1.groupby([\"User_id\", \"Merchant_id\"], as_index=False).agg('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuyanlei/anaconda3/lib/python3.5/site-packages/pandas/util/decorators.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "um = dfoff[[\"User_id\", \"Merchant_id\"]]\n",
    "um.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "um1 = pd.merge(um, tmp, on=[\"User_id\", \"Merchant_id\"], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "um1 = pd.merge(um1, tmp1, on=[\"User_id\", \"Merchant_id\"], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=pd.read_csv(\"./O2O-Coupon-Usage-Forecast/code/wepon/season one/data/dataset1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1, -1])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfoff[\"label\"]= dfoff.apply(get_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuyanlei/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dfoff = coupon_feature(dfoff)\n",
    "dftest =coupon_feature(dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#正确的做法应该是划分数据集，而且至少要划分二分，第一份和第二份时间相邻，作为交叉预测，每一份都包含训练数据集和特征集\n",
    "#[07.01,07.30]\n",
    "dataset3 = dftest\n",
    "#[03.15, 06.30]\n",
    "feature3 = dfoff[((dfoff.Date >= \"20160315\") & (dfoff.Date <=\"20160630\")) | ((dfoff.Date == \"null\") &((dfoff.Date_received >= \"20160315\") & (dfoff.Date_received<=\"20160630\")))]\n",
    "#\n",
    "\n",
    "#[05.15, 06.15]\n",
    "dataset2 = dfoff[(dfoff.Date_received >= \"20160515\") & (dfoff.Date_received <=\"20160615\")]\n",
    "feature2 = dfoff[((dfoff.Date >= \"20160201\") & (dfoff.Date <=\"20160514\")) | ((dfoff.Date == \"null\") &((dfoff.Date_received >= \"20160201\") & (dfoff.Date_received<=\"20160514\")))]\n",
    "\n",
    "#[04.14, 05.14]\n",
    "dataset1 = dfoff[(dfoff.Date_received >= \"20160414\") & (dfoff.Date_received <=\"20160514\")]\n",
    "feature1 = dfoff[((dfoff.Date >= \"20160101\") & (dfoff.Date <=\"20160413\")) | ((dfoff.Date == \"null\") &((dfoff.Date_received >= \"20160101\") & (dfoff.Date_received<=\"20160514\")))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#feature = dfoff[(dfoff['Date'] < '20160516') | ((dfoff['Date'] == 'null') & (dfoff['Date_received'] < '20160516'))].copy()\n",
    "#data = dfoff[(dfoff['Date_received'] >= '20160516') & (dfoff['Date_received'] <= '20160615')].copy()\n",
    "#print(data['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuyanlei/anaconda3/lib/python3.5/site-packages/pandas/util/decorators.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return func(*args, **kwargs)\n",
      "/home/liuyanlei/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:207: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/liuyanlei/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:237: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/liuyanlei/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:242: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/liuyanlei/anaconda3/lib/python3.5/site-packages/pandas/core/generic.py:2701: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n",
      "/home/liuyanlei/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:274: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/liuyanlei/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:279: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#对于feature生成\n",
    "dataset1 = featureProcess(feature1, dataset1, dataset1)\n",
    "dataset2 = featureProcess(feature2, dataset2, dataset2)\n",
    "dataset3 = featureProcess(feature3, dataset3, dataset3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:136301, val size:257126, data size:393427\n",
      "Fitting 1 folds for each of 9 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   9 | elapsed:   52.5s remaining:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   54.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.810392889215\n",
      "{'en__alpha': 0.1, 'en__l1_ratio': 0.1}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.cross_validation import PredefinedSplit\n",
    "import numpy as np\n",
    "import pickle\n",
    "def check_model(train, valid, predictors):\n",
    "    #\n",
    "    \n",
    "    classifier = lambda: SGDClassifier(\n",
    "        loss='log', \n",
    "        penalty='elasticnet', \n",
    "        fit_intercept=True, \n",
    "        max_iter=100, \n",
    "        shuffle=True, \n",
    "        n_jobs=1,\n",
    "        class_weight=None)\n",
    "\n",
    "    model = Pipeline(steps=[\n",
    "        ('ss', StandardScaler()),\n",
    "        ('en', classifier())\n",
    "    ])\n",
    "\n",
    "    parameters = {\n",
    "        'en__alpha': [ 0.001, 0.01, 0.1],\n",
    "        'en__l1_ratio': [ 0.001, 0.01, 0.1]\n",
    "    }\n",
    "    #训练集是train+valid\n",
    "    data = pd.concat([train, valid])\n",
    "    #print(data[predictors].head())\n",
    "    print(\"train size:%s, val size:%s, data size:%s\" %(train.shape[0], valid.shape[0], data.shape[0]))\n",
    "    #生成验证集\n",
    "    index = np.zeros(data.shape[0])\n",
    "    index[:train.shape[0]] = -1\n",
    "    ps = PredefinedSplit(test_fold=index)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        parameters, \n",
    "        cv=ps, \n",
    "        n_jobs=-1, \n",
    "        verbose=1,\n",
    "        scoring='roc_auc')\n",
    "    grid_search = grid_search.fit(data[predictors], \n",
    "                                  data['label'])\n",
    "    \n",
    "    return grid_search\n",
    "\n",
    "predictors = [x for x in dataset1.columns.tolist()if x.startswith(\"c_\") or x.startswith(\"u_\") or x.startswith(\"m_\") or x.startswith(\"um_\") or x.startswith(\"t_\")]\n",
    "\n",
    "#使用dataset1和dataset2做交叉验证,验证集使用固定的dataset2\n",
    "model = check_model(dataset1, dataset2, predictors)\n",
    "print(model.best_score_)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.724999083537\n"
     ]
    }
   ],
   "source": [
    "#全量0.647\n",
    "#去除交叉:0.6445\n",
    "dataset2['pred_prob'] = model.predict_proba(dataset2[predictors])[:,1]\n",
    "validgroup = dataset2.groupby(['Coupon_id'])\n",
    "aucs = []\n",
    "for i in validgroup:\n",
    "    tmpdf = i[1] \n",
    "    if len(tmpdf['label'].unique()) != 2:\n",
    "        continue\n",
    "    fpr, tpr, thresholds = roc_curve(tmpdf['label'], tmpdf['pred_prob'], pos_label=1)\n",
    "    aucs.append(auc(fpr, tpr))\n",
    "print(np.average(aucs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "dataset1.to_csv(\"dataset1.csv\")\n",
    "dataset2.to_csv(\"dataset2.csv\")\n",
    "dataset3.to_csv(\"dataset3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c_discount_rate', 'c_is_manjian', 'c_man', 'c_jian', 'c_distance', 'c_dis_0', 'c_dis_1', 'c_dis_2', 'c_dis_3', 'c_dis_4', 'c_dis_5', 'c_dis_6', 'c_dis_7', 'c_dis_8', 'c_dis_9', 'c_dis_10', 'c_weektype', 'c_weekday_1', 'c_weekday_2', 'c_weekday_3', 'c_weekday_4', 'c_weekday_5', 'c_weekday_6', 'c_weekday_7', 'u_buy_count', 'u_coupon_buy_count', 'u_coupon_count', 'u_coupon_buy_rate', 'u_coupon_use_rate', 'u_mean_distance', 'u_max_distance', 'u_min_distance', 'u_median_distance', 'u_merchant_count', 'u_date_gap_mean', 'u_date_gap_min', 'u_date_gap_max', 'm_buy_count', 'm_coupon_buy_count', 'm_coupon_count', 'm_coupon_buy_rate', 'm_coupon_use_rate', 'm_mean_distance', 'm_max_distance', 'm_min_distance', 'm_median_distance', 'm_date_gap_mean', 'm_date_gap_min', 'm_date_gap_max', 'um_buy_count', 'um_buy_with_coupon_count', 'um_coupon_count', 'um_count', 'um_common_buy_count', 'um_buy_rate', 'um_buy_with_coupon_rate', 'um_coupon_transfer_rate', 'um_common_buy_rate', 't_total_coupon_count', 't_same_coupon_count', 't_same_coupon_morethan1', 't_user_receive_same_coupon_lastone', 't_user_receive_same_coupon_firstone', 't_day_received_all_coupon_num', 't_day_received_same_coupon_num', 't_day_gap_before', 't_day_gap_after']\n",
      "train size:136301, val size:257126, data size:393427\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-245d389d9f7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_gbdt_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-134-245d389d9f7e>\u001b[0m in \u001b[0;36mcheck_gbdt_model\u001b[0;34m(train, valid, predictors)\u001b[0m\n\u001b[1;32m     56\u001b[0m         scoring='roc_auc')\n\u001b[1;32m     57\u001b[0m     grid_search = grid_search.fit(data[predictors], \n\u001b[0;32m---> 58\u001b[0;31m                                   data['label'])\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liuyanlei/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    737\u001b[0m                 **self.best_params_)\n\u001b[1;32m    738\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liuyanlei/anaconda3/lib/python3.5/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liuyanlei/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[0;32m-> 1034\u001b[0;31m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liuyanlei/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[1;32m   1088\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m                                      X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liuyanlei/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m--> 788\u001b[0;31m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liuyanlei/anaconda3/lib/python3.5/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liuyanlei/anaconda3/lib/python3.5/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "def check_gbdt_model(train, valid, predictors):\n",
    "    \n",
    "    classifier = lambda: GradientBoostingClassifier(\n",
    "        random_state=100,\n",
    "        learning_rate=0.1,\n",
    "        n_estimators = 800,\n",
    "        #\n",
    "        max_depth  = 6,\n",
    "        min_samples_leaf=3,\n",
    "        min_samples_split = 900,\n",
    "        ##并没有variace，所以subsample使用1，max_feature使用none\n",
    "        max_features = 'sqrt',  \n",
    "        subsample=0.8,\n",
    "        )\n",
    "\n",
    "    model = Pipeline(steps=[\n",
    "        ('en', classifier())\n",
    "    ])\n",
    "\n",
    "    parameters = {\n",
    "        #不均匀分类问题，调\n",
    "        #'en__max_depth':range(3,8,1),\n",
    "        #'en__min_samples_leaf':range(1,400,10),\n",
    "        #'en__min_samples_split':range(500,2000,100),\n",
    "        #'en__max_features':range(3,8,1),\n",
    "        #'en__subsample'\n",
    "        #先设置初始化值\n",
    "        #'en__n_estimators':range(50,1000,50),\n",
    "    }\n",
    "\n",
    "    #训练集是train+valid\n",
    "    data = pd.concat([train, valid])\n",
    "    #print(data[predictors].head())\n",
    "    print(\"train size:%s, val size:%s, data size:%s\" %(train.shape[0], valid.shape[0], data.shape[0]))\n",
    "    #生成验证集\n",
    "    index = np.zeros(data.shape[0])\n",
    "    index[:train.shape[0]] = -1\n",
    "    ps = PredefinedSplit(test_fold=index)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        parameters, \n",
    "        cv=ps, \n",
    "        n_jobs=-1, \n",
    "        verbose=1,\n",
    "        scoring='roc_auc')\n",
    "    grid_search = grid_search.fit(data[predictors], \n",
    "                                  data['label'])\n",
    "     \n",
    "    return grid_search\n",
    "\n",
    "predictors = [x for x in dataset1.columns.tolist()if x.startswith(\"c_\") or x.startswith(\"u_\") or x.startswith(\"m_\") or x.startswith(\"um_\") or x.startswith(\"t_\")]\n",
    "print(predictors)\n",
    "\n",
    "model = check_gbdt_model(dataset1, dataset2, predictors)\n",
    "print(model.best_score_)\n",
    "print(model.best_params_)\n",
    "#训练集合的auc\n",
    "dataset1['pred_prob'] = model.predict_proba(dataset1[predictors])[:,1]\n",
    "traingroup = dataset1.groupby(['Coupon_id'])\n",
    "aucs = []\n",
    "for i in traingroup:\n",
    "    tmpdf = i[1] \n",
    "    if len(tmpdf['label'].unique()) != 2:\n",
    "        continue\n",
    "    fpr, tpr, thresholds = roc_curve(tmpdf['label'], tmpdf['pred_prob'], pos_label=1)\n",
    "    aucs.append(auc(fpr, tpr))\n",
    "print(\"train set auc:\",np.average(aucs))\n",
    "\n",
    "#全量0.647\n",
    "#去除交叉:0.6445\n",
    "dataset2['pred_prob'] = model.predict_proba(dataset2[predictors])[:,1]\n",
    "validgroup = dataset2.groupby(['Coupon_id'])\n",
    "aucs = []\n",
    "for i in validgroup:\n",
    "    tmpdf = i[1] \n",
    "    if len(tmpdf['label'].unique()) != 2:\n",
    "        continue\n",
    "    fpr, tpr, thresholds = roc_curve(tmpdf['label'], tmpdf['pred_prob'], pos_label=1)\n",
    "    aucs.append(auc(fpr, tpr))\n",
    "print(\"val set auc:\", np.average(aucs))\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#feat_imp = pd.Series(model.best_estimator_.steps[0][1].feature_importances_ , predictors).sort_values(ascending=False)\n",
    "#feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Coupon_id</th>\n",
       "      <th>Date_received</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4129537</td>\n",
       "      <td>9983</td>\n",
       "      <td>20160712</td>\n",
       "      <td>0.010821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6949378</td>\n",
       "      <td>3429</td>\n",
       "      <td>20160706</td>\n",
       "      <td>0.224893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2166529</td>\n",
       "      <td>6928</td>\n",
       "      <td>20160727</td>\n",
       "      <td>0.006805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2166529</td>\n",
       "      <td>1808</td>\n",
       "      <td>20160727</td>\n",
       "      <td>0.007352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6172162</td>\n",
       "      <td>6500</td>\n",
       "      <td>20160708</td>\n",
       "      <td>0.044469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_id  Coupon_id  Date_received  Probability\n",
       "0  4129537       9983       20160712     0.010821\n",
       "1  6949378       3429       20160706     0.224893\n",
       "2  2166529       6928       20160727     0.006805\n",
       "3  2166529       1808       20160727     0.007352\n",
       "4  6172162       6500       20160708     0.044469"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#预测测试数据，进行提交\n",
    "y_test_pred = model.predict_proba(test[predictors])\n",
    "dftest1 = test[['User_id','Coupon_id','Date_received']].copy()\n",
    "dftest1['Probability'] = y_test_pred[:,1]\n",
    "dftest1.to_csv('sample_submission.csv', index=False, header=False)\n",
    "dftest1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4129537,9983,20160712,0.010820877678602155\r\n",
      "6949378,3429,20160706,0.22489285653565008\r\n",
      "2166529,6928,20160727,0.0068049916076474365\r\n"
     ]
    }
   ],
   "source": [
    "#test=coupon_feature(test)\n",
    "!head -3 sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a= pd.read_csv(\"./data/dataset1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b=pd.read_csv(\"O2O-Coupon-Usage-Forecast/code/wepon/season one/data/dataset1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>User_id</th>\n",
       "      <th>Merchant_id</th>\n",
       "      <th>Coupon_id</th>\n",
       "      <th>Discount_rate</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Date_received</th>\n",
       "      <th>Date</th>\n",
       "      <th>label</th>\n",
       "      <th>c_discount_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>um_common_buy_rate</th>\n",
       "      <th>t_total_coupon_count</th>\n",
       "      <th>t_same_coupon_count</th>\n",
       "      <th>t_same_coupon_morethan1</th>\n",
       "      <th>t_user_receive_same_coupon_lastone</th>\n",
       "      <th>t_user_receive_same_coupon_firstone</th>\n",
       "      <th>t_day_received_all_coupon_num</th>\n",
       "      <th>t_day_received_same_coupon_num</th>\n",
       "      <th>t_day_gap_before</th>\n",
       "      <th>t_day_gap_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1832624</td>\n",
       "      <td>3381</td>\n",
       "      <td>7610</td>\n",
       "      <td>200:20</td>\n",
       "      <td>0</td>\n",
       "      <td>20160429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>163606</td>\n",
       "      <td>1569</td>\n",
       "      <td>5054</td>\n",
       "      <td>200:30</td>\n",
       "      <td>10</td>\n",
       "      <td>20160421</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4061024</td>\n",
       "      <td>3381</td>\n",
       "      <td>7610</td>\n",
       "      <td>200:20</td>\n",
       "      <td>10</td>\n",
       "      <td>20160426</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>106443</td>\n",
       "      <td>450</td>\n",
       "      <td>3732</td>\n",
       "      <td>30:5</td>\n",
       "      <td>0</td>\n",
       "      <td>20160429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>114747</td>\n",
       "      <td>1569</td>\n",
       "      <td>5054</td>\n",
       "      <td>200:30</td>\n",
       "      <td>9</td>\n",
       "      <td>20160426</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  User_id  Merchant_id  Coupon_id Discount_rate  Distance  \\\n",
       "0           0  1832624         3381       7610        200:20         0   \n",
       "1           1   163606         1569       5054        200:30        10   \n",
       "2           2  4061024         3381       7610        200:20        10   \n",
       "3           3   106443          450       3732          30:5         0   \n",
       "4           4   114747         1569       5054        200:30         9   \n",
       "\n",
       "   Date_received  Date  label  c_discount_rate       ...         \\\n",
       "0       20160429     0      0         0.900000       ...          \n",
       "1       20160421     0      0         0.850000       ...          \n",
       "2       20160426     0      0         0.900000       ...          \n",
       "3       20160429     0      0         0.833333       ...          \n",
       "4       20160426     0      0         0.850000       ...          \n",
       "\n",
       "   um_common_buy_rate  t_total_coupon_count  t_same_coupon_count  \\\n",
       "0                 0.0                     1                    1   \n",
       "1                 0.0                     1                    1   \n",
       "2                 0.0                     1                    1   \n",
       "3                 0.0                     1                    1   \n",
       "4                 0.0                     1                    1   \n",
       "\n",
       "   t_same_coupon_morethan1  t_user_receive_same_coupon_lastone  \\\n",
       "0                        0                                  -1   \n",
       "1                        0                                  -1   \n",
       "2                        0                                  -1   \n",
       "3                        0                                  -1   \n",
       "4                        0                                  -1   \n",
       "\n",
       "   t_user_receive_same_coupon_firstone  t_day_received_all_coupon_num  \\\n",
       "0                                   -1                              1   \n",
       "1                                   -1                              1   \n",
       "2                                   -1                              1   \n",
       "3                                   -1                              1   \n",
       "4                                   -1                              1   \n",
       "\n",
       "   t_day_received_same_coupon_num  t_day_gap_before  t_day_gap_after  \n",
       "0                               1                -1               -1  \n",
       "1                               1                -1               -1  \n",
       "2                               1                -1               -1  \n",
       "3                               1                -1               -1  \n",
       "4                               1                -1               -1  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8681161913693833"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.c_discount_rate.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8681161913693833"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.discount_rate.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
